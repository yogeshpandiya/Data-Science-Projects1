{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc2bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #advanced natural language processing,\n",
    "import re #to clean the word \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # to convert work in numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bff710",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm') #nlp pipeline of spacy \n",
    "stopwords = nlp.Defaults.stop_words # fetching all the stopwords \n",
    "stopwords.add('group2') #adding this extra word to stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a80980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('Text_NLP_Viva_QA.txt','r').read() #reading the text file\n",
    "cleaned_file = re.sub('[^\\s\\w]','',file) #removed punctation and widespace \n",
    "cleaned_file = cleaned_file.lower() # lowering the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "254dec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(cleaned_file) #converting spacy doc to text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb658d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [] # lst to append lemmatize tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4044a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in doc: # to get the tokens\n",
    "    if tokens.orth_ not in stopwords: # if tokens not present in stopwords\n",
    "        lst.append(tokens.lemma_) #then lemmatizing them and appending to list name lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d6eafd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " era great scourge air pollution account impact climate change impact public individual \n",
      " health increase morbidity mortality pollutant major factor disease human particulate matter \n",
      " pm particle variable small diameter penetrate respiratory system inhalation cause respiratory cardiovascular disease \n",
      " reproductive central nervous system dysfunction cancer despite fact ozone stratosphere play protective role ultraviolet irradiation \n",
      " harmful high concentration ground level affect respiratory cardiovascular system furthermore nitrogen oxide sulfur dioxide volatile \n",
      " organic compound vocs dioxin polycyclic aromatic hydrocarbon pah consider air pollutant harmful human carbon monoxide provoke \n",
      " direct poisoning breathe high level heavy metal lead absorb human body lead direct poisoning chronic intoxication depend exposure \n",
      "  disease occur aforementione substance include principally respiratory problem chronic obstructive pulmonary disease copd asthma bronchiolitis lung cancer cardiovascular event central nervous system dysfunction cutaneous disease climate change result environmental pollution affect geographical distribution infectious disease natural disaster way tackle problem public awareness couple multidisciplinary approach scientific expert national international organization address emergence threat propose sustainable solution \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_text = ' '.join(lst) # no such use just to print how final text look like after preprocessing\n",
    "print(nlp(final_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365d762f",
   "metadata": {},
   "source": [
    "## For words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "560b5ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF = TfidfVectorizer()  #creating object of TERMED FREQUENCY - INVERSE DOCUMNET FREQUENCY to convert text to numbers\n",
    "arr = TFIDF.fit_transform(lst).toarray()\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53228211",
   "metadata": {},
   "source": [
    "### For Sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bad20818",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst3 = []\n",
    "doc1 = nlp(file) #converting spacy doc to text data\n",
    "for tokens in doc1.sents: # to get the tokens\n",
    "    if tokens.orth_ not in stopwords: # if tokens not present in stopwords\n",
    "        a = tokens.lemma_ #then lemmatizing them and appending to list name lst\n",
    "        a = re.sub('[^\\s\\w]','',a) # romving punctuation from sentence\n",
    "        lst3.append(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a3da4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.16360449, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.18047921, ..., 0.18047921, 0.        ,\n",
       "        0.18047921]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDF = TfidfVectorizer()  #creating object of TERMED FREQUENCY - INVERSE DOCUMNET FREQUENCY to convert text to numbers\n",
    "arr = TFIDF.fit_transform(lst3).toarray()\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "155be49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 164)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ffedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5122b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
